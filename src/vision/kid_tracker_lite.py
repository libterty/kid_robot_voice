"""
è¼•é‡ç´šæ•´åˆè¿½è¹¤å™¨ (MediaPipe 0.10.32 Tasks API ç‰ˆæœ¬)
ä½¿ç”¨è¼•é‡ç´šäººè‡‰åµæ¸¬å™¨ + MediaPipe Pose Landmarker
"""

import os
import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
from dotenv import load_dotenv
from .face_detector_lite import LightweightFaceDetector
from pathlib import Path

load_dotenv()


class KidTrackerLite:
    """è¼•é‡ç´šæ•´åˆè¿½è¹¤å™¨ï¼ˆä½¿ç”¨ MediaPipe Tasks APIï¼‰"""
    
    def __init__(self, camera_id: int = None):
        """åˆå§‹åŒ–è¿½è¹¤å™¨"""
        self.camera_id = camera_id if camera_id is not None else \
                         int(os.getenv('CAMERA_ID', '0'))
        
        # åˆå§‹åŒ–è¼•é‡ç´šäººè‡‰åµæ¸¬å™¨
        self.face_detector = LightweightFaceDetector(camera_id=self.camera_id)
        
        # åˆå§‹åŒ– MediaPipe Pose Landmarkerï¼ˆä½¿ç”¨ tasks APIï¼‰
        model_path = self._download_pose_model()
        base_options = python.BaseOptions(model_asset_path=model_path)
        options = vision.PoseLandmarkerOptions(
            base_options=base_options,
            min_pose_detection_confidence=0.5,
            min_pose_presence_confidence=0.5
        )
        self.pose_landmarker = vision.PoseLandmarker.create_from_options(options)
        
        # è¿½è¹¤ç‹€æ…‹
        self.tracking_mode = "fusion"
        self.target_info = {
            'face_position': None,
            'body_position': None,
            'final_position': None,
            'distance': None,
            'confidence': 0,
            'name': None,
            'is_detected': False
        }
        
        self.cap = None
        
        print("âœ… ä½¿ç”¨è¼•é‡ç´šè¿½è¹¤å™¨ï¼ˆMediaPipe 0.10.32 Tasks APIï¼‰")
    
    def _download_pose_model(self):
        """ä¸‹è¼‰ Pose Landmarker æ¨¡å‹"""
        model_path = Path("models/pose_landmarker.task")
        model_path.parent.mkdir(exist_ok=True)
        
        if not model_path.exists():
            print("ğŸ“¥ ä¸‹è¼‰ Pose Landmarker æ¨¡å‹...")
            import urllib.request
            url = "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task"
            urllib.request.urlretrieve(url, model_path)
            print("âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
        
        return str(model_path)
    
    def start_camera(self):
        """å•Ÿå‹•æ”åƒé ­"""
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            raise Exception(f"âŒ ç„¡æ³•é–‹å•Ÿæ”åƒé ­ {self.camera_id}")
        
        self.face_detector.cap = self.cap
        print(f"âœ… æ”åƒé ­ {self.camera_id} å·²å•Ÿå‹•")
    
    def detect_body(self, frame):
        """åµæ¸¬äººé«”éª¨æ¶ï¼ˆä½¿ç”¨ Pose Landmarkerï¼‰"""
        # è½‰æ›ç‚º MediaPipe Image
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # åŸ·è¡Œåµæ¸¬
        detection_result = self.pose_landmarker.detect(mp_image)
        
        body_center = None
        distance_estimate = None
        pose_landmarks = None
        
        if detection_result.pose_landmarks:
            # å–ç¬¬ä¸€å€‹äººçš„éª¨æ¶
            landmarks = detection_result.pose_landmarks[0]
            pose_landmarks = landmarks
            
            h, w, _ = frame.shape
            
            # è¨ˆç®—èº«é«”ä¸­å¿ƒ
            # Pose landmarks ç´¢å¼•ï¼š0=é¼»å­, 23=å·¦é«–, 24=å³é«–
            nose = landmarks[0]
            left_hip = landmarks[23]
            right_hip = landmarks[24]
            
            center_x = int((nose.x + (left_hip.x + right_hip.x) / 2) / 2 * w)
            center_y = int((nose.y + (left_hip.y + right_hip.y) / 2) / 2 * h)
            body_center = (center_x, center_y)
            
            # è·é›¢ä¼°ç®—ï¼ˆåŸºæ–¼è‚©å¯¬ï¼‰
            # 11=å·¦è‚©, 12=å³è‚©
            left_shoulder = landmarks[11]
            right_shoulder = landmarks[12]
            shoulder_width_px = abs(left_shoulder.x - right_shoulder.x) * w
            
            if shoulder_width_px > 0:
                distance_estimate = (40 * 600) / shoulder_width_px
        
        return body_center, distance_estimate, pose_landmarks
    
    def fuse_positions(self, face_pos, body_pos):
        """èåˆäººè‡‰å’Œèº«é«”ä½ç½®"""
        if face_pos and body_pos:
            x = int(face_pos[0] * 0.7 + body_pos[0] * 0.3)
            y = int(face_pos[1] * 0.7 + body_pos[1] * 0.3)
            return (x, y)
        elif face_pos:
            return face_pos
        elif body_pos:
            return body_pos
        return None
    
    def track_target(self, frame):
        """åŸ·è¡Œå®Œæ•´çš„ç›®æ¨™è¿½è¹¤"""
        annotated_frame = frame.copy()
        h, w, _ = frame.shape
        
        # 1. äººè‡‰è¾¨è­˜
        face_frame, face_detections = self.face_detector.detect_and_recognize(frame)
        
        # 2. èº«é«”åµæ¸¬
        body_center, distance, pose_landmarks = self.detect_body(frame)
        
        # 3. æ‰¾å‡ºç›®æ¨™
        target_face_pos = None
        target_name = None
        target_confidence = 0
        
        if self.face_detector.target_name:
            for name, confidence, (x, y, w, h) in face_detections:
                if name == self.face_detector.target_name:
                    target_face_pos = (x + w//2, y + h//2)
                    target_name = name
                    target_confidence = confidence
                    break
        
        # 4. èåˆä½ç½®
        final_position = self.fuse_positions(target_face_pos, body_center)
        
        # 5. æ›´æ–°ç‹€æ…‹
        self.target_info = {
            'face_position': target_face_pos,
            'body_position': body_center,
            'final_position': final_position,
            'distance': distance,
            'confidence': target_confidence,
            'name': target_name,
            'is_detected': final_position is not None
        }
        
        # 6. è¦–è¦ºåŒ–
        annotated_frame = face_frame.copy()
        
        # ç¹ªè£½éª¨æ¶
        if pose_landmarks:
            self._draw_pose_landmarks(annotated_frame, pose_landmarks, w, h)
        
        # ç¹ªè£½èº«é«”ä¸­å¿ƒ
        if body_center:
            cv2.circle(annotated_frame, body_center, 8, (255, 0, 255), -1)
            cv2.putText(annotated_frame, "Body", 
                       (body_center[0] - 25, body_center[1] - 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)
        
        # ç¹ªè£½æœ€çµ‚è¿½è¹¤é»
        if final_position:
            cv2.circle(annotated_frame, final_position, 15, (0, 255, 0), 3)
            cv2.circle(annotated_frame, final_position, 5, (0, 255, 0), -1)
            
            # åå­—æº–æ˜Ÿ
            cv2.line(annotated_frame, 
                    (final_position[0] - 20, final_position[1]),
                    (final_position[0] + 20, final_position[1]),
                    (0, 255, 0), 2)
            cv2.line(annotated_frame, 
                    (final_position[0], final_position[1] - 20),
                    (final_position[0], final_position[1] + 20),
                    (0, 255, 0), 2)
        
        # è³‡è¨Šé¢æ¿
        self._draw_info_panel(annotated_frame, w, h)
        
        return annotated_frame, self.target_info
    
    def _draw_pose_landmarks(self, frame, landmarks, width, height):
        """ç¹ªè£½äººé«”éª¨æ¶"""
        # Pose é€£æ¥å®šç¾©
        connections = [
            (0, 1), (1, 2), (2, 3), (3, 7),  # é ­éƒ¨
            (0, 4), (4, 5), (5, 6), (6, 8),  # é ­éƒ¨
            (9, 10),  # å˜´å·´
            (11, 12),  # è‚©è†€
            (11, 13), (13, 15),  # å·¦æ‰‹è‡‚
            (12, 14), (14, 16),  # å³æ‰‹è‡‚
            (11, 23), (12, 24),  # è»€å¹¹
            (23, 24),  # é«–éƒ¨
            (23, 25), (25, 27),  # å·¦è…¿
            (24, 26), (26, 28),  # å³è…¿
        ]
        
        # ç¹ªè£½é€£ç·š
        for start_idx, end_idx in connections:
            if start_idx < len(landmarks) and end_idx < len(landmarks):
                start = landmarks[start_idx]
                end = landmarks[end_idx]
                
                start_point = (int(start.x * width), int(start.y * height))
                end_point = (int(end.x * width), int(end.y * height))
                
                cv2.line(frame, start_point, end_point, (0, 255, 0), 2)
        
        # ç¹ªè£½é—œéµé»
        for landmark in landmarks:
            point = (int(landmark.x * width), int(landmark.y * height))
            cv2.circle(frame, point, 3, (0, 255, 255), -1)
    
    def _draw_info_panel(self, frame, width, height):
        """ç¹ªè£½è³‡è¨Šé¢æ¿"""
        cv2.rectangle(frame, (10, 10), (400, 180), (0, 0, 0), -1)
        cv2.rectangle(frame, (10, 10), (400, 180), (0, 255, 0), 2)
        
        y_offset = 35
        line_height = 25
        
        cv2.putText(frame, "Kid Robot Tracker (Tasks API)", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        y_offset += line_height
        
        if self.target_info['is_detected']:
            status = f"Status: TRACKING ({self.tracking_mode.upper()})"
            color = (0, 255, 0)
        else:
            status = "Status: SEARCHING..."
            color = (0, 165, 255)
        
        cv2.putText(frame, status, (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        y_offset += line_height
        
        if self.target_info['name']:
            cv2.putText(frame, f"Target: {self.target_info['name']}", (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        else:
            target_name = self.face_detector.target_name or "None"
            cv2.putText(frame, f"Target: {target_name}", (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
        y_offset += line_height
        
        if self.target_info['confidence'] > 0:
            cv2.putText(frame, f"Confidence: {self.target_info['confidence']:.1f}%", 
                       (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        y_offset += line_height
        
        if self.target_info['distance']:
            cv2.putText(frame, f"Distance: {self.target_info['distance']:.0f} cm", 
                       (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        y_offset += line_height
        
        if self.target_info['final_position']:
            pos = self.target_info['final_position']
            cv2.putText(frame, f"Position: ({pos[0]}, {pos[1]})", (20, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def set_tracking_mode(self, mode: str):
        """è¨­å®šè¿½è¹¤æ¨¡å¼"""
        if mode in ["face", "body", "fusion"]:
            self.tracking_mode = mode
            print(f"ğŸ¯ è¿½è¹¤æ¨¡å¼å·²è¨­å®šç‚º: {mode}")
        else:
            print(f"âŒ ç„¡æ•ˆçš„è¿½è¹¤æ¨¡å¼: {mode}")
    
    def run_tracking(self):
        """åŸ·è¡Œå³æ™‚è¿½è¹¤"""
        if self.cap is None:
            self.start_camera()
        
        print("\n" + "="*60)
        print("å®¶åº­é™ªè®€æ©Ÿå™¨äºº - è¿½è¹¤ç³»çµ± (Tasks API)")
        print("="*60)
        print("\næŒ‰éµèªªæ˜ï¼š")
        print("  'q' - é€€å‡º")
        print("  'r' - è¨»å†Šæ–°äººè‡‰")
        print("  't' - è¨­å®šè¿½è¹¤ç›®æ¨™")
        print("  '1' - äººè‡‰è¿½è¹¤æ¨¡å¼")
        print("  '2' - èº«é«”è¿½è¹¤æ¨¡å¼")
        print("  '3' - èåˆè¿½è¹¤æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰")
        print("  's' - é¡¯ç¤ºç‹€æ…‹")
        print()
        
        if self.face_detector.target_name:
            print(f"ğŸ¯ ç•¶å‰è¿½è¹¤ç›®æ¨™: {self.face_detector.target_name}")
        else:
            print("âš ï¸ å°šæœªè¨­å®šè¿½è¹¤ç›®æ¨™ï¼Œè«‹æŒ‰ 't' è¨­å®š")
        
        print(f"ğŸ“Š è¿½è¹¤æ¨¡å¼: {self.tracking_mode}\n")
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ ç„¡æ³•è®€å–å½±åƒ")
                break
            
            tracked_frame, target_info = self.track_target(frame)
            cv2.imshow('Kid Robot - Tracker (Tasks API)', tracked_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('r'):
                name = input("\nè«‹è¼¸å…¥è¦è¨»å†Šçš„åå­—: ")
                if name:
                    self.face_detector.register_face(name)
            elif key == ord('t'):
                print(f"\nå·²è¨»å†Šçš„äººè‡‰: {list(self.face_detector.known_faces.keys())}")
                target = input("è«‹è¼¸å…¥è¦è¿½è¹¤çš„åå­—: ")
                if target:
                    self.face_detector.set_target(target)
            elif key == ord('1'):
                self.set_tracking_mode("face")
            elif key == ord('2'):
                self.set_tracking_mode("body")
            elif key == ord('3'):
                self.set_tracking_mode("fusion")
            elif key == ord('s'):
                self._print_status()
        
        self.stop_camera()
    
    def _print_status(self):
        """å°å‡ºç‹€æ…‹"""
        print("\n" + "="*60)
        print("ç•¶å‰è¿½è¹¤ç‹€æ…‹")
        print("="*60)
        print(f"è¿½è¹¤æ¨¡å¼: {self.tracking_mode}")
        print(f"ç›®æ¨™åç¨±: {self.target_info['name'] or 'ç„¡'}")
        print(f"åµæ¸¬ç‹€æ…‹: {'âœ… å·²åµæ¸¬' if self.target_info['is_detected'] else 'âŒ æœªåµæ¸¬'}")
        print(f"ä¿¡å¿ƒåº¦: {self.target_info['confidence']:.1f}%")
        print(f"è·é›¢: {self.target_info['distance']:.0f} cm" if self.target_info['distance'] else "è·é›¢: N/A")
        print(f"æœ€çµ‚ä½ç½®: {self.target_info['final_position']}")
        print("="*60 + "\n")
    
    def stop_camera(self):
        """åœæ­¢æ”åƒé ­"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        print("âœ… æ”åƒé ­å·²é—œé–‰")
    
    def get_tracking_info(self):
        """å–å¾—è¿½è¹¤è³‡è¨Š"""
        return self.target_info


if __name__ == "__main__":
    tracker = KidTrackerLite()
    tracker.run_tracking()