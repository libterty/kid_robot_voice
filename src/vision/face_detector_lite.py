"""
è¼•é‡ç´šäººè‡‰åµæ¸¬æ¨¡çµ„ (MediaPipe 0.10.32 tasks API ç‰ˆæœ¬)
ä½¿ç”¨ MediaPipe Tasks é€²è¡Œäººè‡‰åµæ¸¬èˆ‡ç°¡å–®è¾¨è­˜
ä¸ä¾è³´ face_recognition å¥—ä»¶
"""

import os
import cv2
import numpy as np
import pickle
from pathlib import Path
from dotenv import load_dotenv
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

load_dotenv()


class LightweightFaceDetector:
    """è¼•é‡ç´šäººè‡‰åµæ¸¬èˆ‡è¾¨è­˜å™¨ï¼ˆä½¿ç”¨ MediaPipe Tasks APIï¼‰"""
    
    def __init__(self, camera_id: int = None, database_path: str = "face_database"):
        """
        åˆå§‹åŒ–äººè‡‰åµæ¸¬å™¨
        
        Args:
            camera_id: æ”åƒé ­ ID
            database_path: äººè‡‰è³‡æ–™åº«å„²å­˜è·¯å¾‘
        """
        self.camera_id = camera_id if camera_id is not None else \
                         int(os.getenv('CAMERA_ID', '0'))
        
        # åˆå§‹åŒ– MediaPipe Face Detectorï¼ˆä½¿ç”¨ tasks APIï¼‰
        base_options = python.BaseOptions(
            model_asset_path=self._download_face_detector_model()
        )
        options = vision.FaceDetectorOptions(
            base_options=base_options,
            min_detection_confidence=0.5
        )
        self.face_detector = vision.FaceDetector.create_from_options(options)
        
        # åˆå§‹åŒ– Face Landmarkerï¼ˆç”¨æ–¼ç‰¹å¾µæå–ï¼‰
        landmarker_options = vision.FaceLandmarkerOptions(
            base_options=python.BaseOptions(
                model_asset_path=self._download_face_landmarker_model()
            ),
            min_face_detection_confidence=0.5,
            min_face_presence_confidence=0.5
        )
        self.face_landmarker = vision.FaceLandmarker.create_from_options(landmarker_options)
        
        # äººè‡‰è³‡æ–™åº«
        self.database_path = Path(database_path)
        self.database_path.mkdir(exist_ok=True)
        self.known_faces = {}
        self.load_database()
        
        # æ”åƒé ­
        self.cap = None
        
        # è¿½è¹¤ç‹€æ…‹
        self.target_name = None
        self.last_seen_position = None
        
        print("âœ… ä½¿ç”¨è¼•é‡ç´šäººè‡‰åµæ¸¬å™¨ï¼ˆMediaPipe 0.10.32 Tasks APIï¼‰")
    
    def _download_face_detector_model(self):
        """ä¸‹è¼‰ Face Detector æ¨¡å‹"""
        model_path = Path("models/face_detector.tflite")
        model_path.parent.mkdir(exist_ok=True)
        
        if not model_path.exists():
            print("ğŸ“¥ ä¸‹è¼‰ Face Detector æ¨¡å‹...")
            import urllib.request
            url = "https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
            urllib.request.urlretrieve(url, model_path)
            print("âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
        
        return str(model_path)
    
    def _download_face_landmarker_model(self):
        """ä¸‹è¼‰ Face Landmarker æ¨¡å‹"""
        model_path = Path("models/face_landmarker.task")
        model_path.parent.mkdir(exist_ok=True)
        
        if not model_path.exists():
            print("ğŸ“¥ ä¸‹è¼‰ Face Landmarker æ¨¡å‹...")
            import urllib.request
            url = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
            urllib.request.urlretrieve(url, model_path)
            print("âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
        
        return str(model_path)
    
    def load_database(self):
        """è¼‰å…¥å·²è¨»å†Šçš„äººè‡‰è³‡æ–™"""
        db_file = self.database_path / "faces_tasks.pkl"
        if db_file.exists():
            with open(db_file, 'rb') as f:
                self.known_faces = pickle.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(self.known_faces)} å€‹äººè‡‰è³‡æ–™")
        else:
            print("â„¹ï¸ äººè‡‰è³‡æ–™åº«ç‚ºç©ºï¼Œè«‹å…ˆè¨»å†Šäººè‡‰")
    
    def save_database(self):
        """å„²å­˜äººè‡‰è³‡æ–™åº«"""
        db_file = self.database_path / "faces_tasks.pkl"
        with open(db_file, 'wb') as f:
            pickle.dump(self.known_faces, f)
        print(f"âœ… äººè‡‰è³‡æ–™åº«å·²å„²å­˜ï¼ˆå…± {len(self.known_faces)} äººï¼‰")
    
    def start_camera(self):
        """å•Ÿå‹•æ”åƒé ­"""
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            raise Exception(f"âŒ ç„¡æ³•é–‹å•Ÿæ”åƒé ­ {self.camera_id}")
        print(f"âœ… æ”åƒé ­ {self.camera_id} å·²å•Ÿå‹•")
    
    def detect_faces(self, frame):
        """
        ä½¿ç”¨ MediaPipe Tasks åµæ¸¬äººè‡‰ä½ç½®
        
        Args:
            frame: OpenCV å½±åƒå¹€
            
        Returns:
            List of face bounding boxes [(x, y, w, h), ...]
        """
        # è½‰æ›ç‚º MediaPipe Image
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # åŸ·è¡Œåµæ¸¬
        detection_result = self.face_detector.detect(mp_image)
        
        faces = []
        if detection_result.detections:
            h, w, _ = frame.shape
            for detection in detection_result.detections:
                bbox = detection.bounding_box
                x = int(bbox.origin_x)
                y = int(bbox.origin_y)
                width = int(bbox.width)
                height = int(bbox.height)
                
                # ç¢ºä¿åº§æ¨™åœ¨æœ‰æ•ˆç¯„åœå…§
                x = max(0, x)
                y = max(0, y)
                width = min(width, w - x)
                height = min(height, h - y)
                
                faces.append((x, y, width, height))
        
        return faces
    
    def extract_face_features(self, frame, bbox):
        """
        æå–äººè‡‰ç‰¹å¾µå‘é‡ï¼ˆä½¿ç”¨ Face Landmarkerï¼‰
        
        Args:
            frame: åŸå§‹å½±åƒ
            bbox: (x, y, w, h) äººè‡‰é‚Šç•Œæ¡†
            
        Returns:
            ç‰¹å¾µå‘é‡ (numpy array) æˆ– None
        """
        x, y, w, h = bbox
        
        # æ“´å±•é‚Šç•Œæ¡†
        padding = int(w * 0.2)
        x1 = max(0, x - padding)
        y1 = max(0, y - padding)
        x2 = min(frame.shape[1], x + w + padding)
        y2 = min(frame.shape[0], y + h + padding)
        
        # è£åˆ‡äººè‡‰å€åŸŸ
        face_roi = frame[y1:y2, x1:x2]
        
        if face_roi.size == 0:
            return None
        
        # è½‰æ›ç‚º MediaPipe Image
        rgb_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_face)
        
        # ä½¿ç”¨ Face Landmarker æå–ç‰¹å¾µé»
        landmarker_result = self.face_landmarker.detect(mp_image)
        
        if landmarker_result.face_landmarks:
            # å–ç¬¬ä¸€å€‹è‡‰éƒ¨çš„ç‰¹å¾µé»
            face_landmarks = landmarker_result.face_landmarks[0]
            
            # æå–é—œéµç‰¹å¾µé»åº§æ¨™ï¼ˆçœ¼ç›ã€é¼»å­ã€å˜´å·´ï¼‰
            key_indices = [
                # å·¦çœ¼
                33, 133, 160, 159, 158, 157, 173,
                # å³çœ¼
                263, 362, 387, 386, 385, 384, 398,
                # é¼»å­
                1, 2, 98, 327,
                # å˜´å·´
                61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291,
                # è‡‰éƒ¨è¼ªå»“
                10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361,
                234, 127, 162, 21, 54, 103, 67, 109
            ]
            
            features = []
            for idx in key_indices:
                if idx < len(face_landmarks):
                    landmark = face_landmarks[idx]
                    features.extend([landmark.x, landmark.y, landmark.z])
            
            return np.array(features)
        
        # å¦‚æœ Face Landmarker å¤±æ•—ï¼Œä½¿ç”¨é¡è‰²ç›´æ–¹åœ–
        return self._extract_color_histogram(face_roi)
    
    def _extract_color_histogram(self, face_roi):
        """æå–é¡è‰²ç›´æ–¹åœ–ä½œç‚ºå¾Œå‚™ç‰¹å¾µ"""
        hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)
        
        hist_h = cv2.calcHist([hsv], [0], None, [50], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [60], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [60], [0, 256])
        
        cv2.normalize(hist_h, hist_h)
        cv2.normalize(hist_s, hist_s)
        cv2.normalize(hist_v, hist_v)
        
        features = np.concatenate([hist_h.flatten(), hist_s.flatten(), hist_v.flatten()])
        return features
    
    def compare_faces(self, feature1, feature2):
        """
        æ¯”è¼ƒå…©å€‹ç‰¹å¾µå‘é‡çš„ç›¸ä¼¼åº¦
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (0-100, è¶Šé«˜è¶Šç›¸ä¼¼)
        """
        if feature1 is None or feature2 is None:
            return 0
        
        if len(feature1) != len(feature2):
            return 0
        
        # ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(feature1, feature2)
        norm1 = np.linalg.norm(feature1)
        norm2 = np.linalg.norm(feature2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        similarity = dot_product / (norm1 * norm2)
        return max(0, min(100, (similarity + 1) * 50))
    
    def recognize_face(self, frame, face_location):
        """
        è¾¨è­˜äººè‡‰
        
        Args:
            frame: OpenCV å½±åƒå¹€
            face_location: (x, y, w, h) äººè‡‰ä½ç½®
            
        Returns:
            (name, confidence) æˆ– (None, 0)
        """
        if len(self.known_faces) == 0:
            return None, 0
        
        current_features = self.extract_face_features(frame, face_location)
        
        if current_features is None:
            return None, 0
        
        best_match_name = None
        best_similarity = 0
        
        for name, data in self.known_faces.items():
            for stored_features in data['features']:
                similarity = self.compare_faces(current_features, stored_features)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match_name = name
        
        threshold = 70
        
        if best_similarity >= threshold:
            return best_match_name, best_similarity
        
        return None, 0
    
    def register_face(self, name: str, num_samples: int = 5):
        """è¨»å†Šæ–°äººè‡‰"""
        if self.cap is None:
            self.start_camera()
        
        print(f"\nğŸ“¸ é–‹å§‹è¨»å†Š '{name}' çš„äººè‡‰")
        print(f"è«‹ä¿æŒæ­£é¢å°é¡é ­ï¼Œå°‡æ‹æ” {num_samples} å¼µç…§ç‰‡")
        print("æŒ‰ SPACE æ‹ç…§ï¼ŒæŒ‰ ESC å–æ¶ˆ\n")
        
        features_list = []
        images_list = []
        captured = 0
        
        while captured < num_samples:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            faces = self.detect_faces(frame)
            
            display_frame = frame.copy()
            for (x, y, w, h) in faces:
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            cv2.putText(
                display_frame,
                f"Captured: {captured}/{num_samples} - Press SPACE",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 255, 0),
                2
            )
            
            cv2.imshow('Register Face', display_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord(' ') and len(faces) > 0:
                bbox = faces[0]
                features = self.extract_face_features(frame, bbox)
                
                if features is not None:
                    features_list.append(features)
                    x, y, w, h = bbox
                    face_img = frame[y:y+h, x:x+w]
                    images_list.append(face_img)
                    captured += 1
                    print(f"âœ… å·²æ‹æ” {captured}/{num_samples}")
                else:
                    print("âš ï¸ ç‰¹å¾µæå–å¤±æ•—ï¼Œè«‹é‡è©¦")
            
            elif key == 27:
                print("âŒ è¨»å†Šå·²å–æ¶ˆ")
                cv2.destroyWindow('Register Face')
                return
        
        self.known_faces[name] = {
            'features': features_list,
            'images': images_list
        }
        self.save_database()
        
        print(f"\nğŸ‰ '{name}' è¨»å†ŠæˆåŠŸï¼")
        cv2.destroyWindow('Register Face')
    
    def set_target(self, name: str):
        """è¨­å®šè¦è¿½è¹¤çš„ç›®æ¨™"""
        if name in self.known_faces:
            self.target_name = name
            print(f"ğŸ¯ è¿½è¹¤ç›®æ¨™è¨­å®šç‚º: {name}")
        else:
            print(f"âŒ æ‰¾ä¸åˆ° '{name}'ï¼Œè«‹å…ˆè¨»å†Šäººè‡‰")
    
    def detect_and_recognize(self, frame):
        """
        åµæ¸¬ä¸¦è¾¨è­˜ç•«é¢ä¸­çš„æ‰€æœ‰äººè‡‰
        
        Returns:
            (annotated_frame, detections)
        """
        faces = self.detect_faces(frame)
        
        detections = []
        annotated_frame = frame.copy()
        
        for face_location in faces:
            x, y, w, h = face_location
            
            name, confidence = self.recognize_face(frame, face_location)
            
            if name == self.target_name:
                color = (0, 255, 0)
                label = f"{name} ({confidence:.1f}%) [TARGET]"
                self.last_seen_position = (x + w//2, y + h//2)
            elif name:
                color = (255, 0, 0)
                label = f"{name} ({confidence:.1f}%)"
            else:
                color = (0, 0, 255)
                label = "Unknown"
            
            cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(
                annotated_frame,
                label,
                (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                color,
                2
            )
            
            detections.append((name, confidence, face_location))
        
        return annotated_frame, detections
    
    def run_live_recognition(self):
        """åŸ·è¡Œå³æ™‚äººè‡‰è¾¨è­˜"""
        if self.cap is None:
            self.start_camera()
        
        print("\nğŸ¥ å³æ™‚äººè‡‰è¾¨è­˜å•Ÿå‹•ï¼")
        print("æŒ‰éµèªªæ˜ï¼š")
        print("  'q' - é€€å‡º")
        print("  'r' - è¨»å†Šæ–°äººè‡‰")
        print("  't' - è¨­å®šè¿½è¹¤ç›®æ¨™")
        if self.target_name:
            print(f"\nğŸ¯ ç•¶å‰è¿½è¹¤ç›®æ¨™: {self.target_name}\n")
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ ç„¡æ³•è®€å–å½±åƒ")
                break
            
            annotated_frame, detections = self.detect_and_recognize(frame)
            
            cv2.imshow('Kid Robot - Face Recognition (Tasks API)', annotated_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('r'):
                name = input("\nè«‹è¼¸å…¥è¦è¨»å†Šçš„åå­—: ")
                if name:
                    self.register_face(name)
            elif key == ord('t'):
                print(f"\nå·²è¨»å†Šçš„äººè‡‰: {list(self.known_faces.keys())}")
                target = input("è«‹è¼¸å…¥è¦è¿½è¹¤çš„åå­—: ")
                if target:
                    self.set_target(target)
        
        self.stop_camera()
    
    def stop_camera(self):
        """åœæ­¢æ”åƒé ­"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        print("âœ… æ”åƒé ­å·²é—œé–‰")
    
    def get_target_position(self):
        """å–å¾—è¿½è¹¤ç›®æ¨™çš„ä½ç½®"""
        return self.last_seen_position


if __name__ == "__main__":
    print("\n" + "="*60)
    print("å®¶åº­é™ªè®€æ©Ÿå™¨äºº - äººè‡‰è¾¨è­˜ç³»çµ± (Tasks API)")
    print("="*60)
    
    detector = LightweightFaceDetector()
    detector.run_live_recognition()