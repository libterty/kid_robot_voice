"""
LLM å°è©±å¼•æ“æ¨¡çµ„
æ”¯æ´ Ollama (æœ¬åœ°) å’Œ Gemini (é›²ç«¯)
æ”¯æ´ Multi-Agent æ¨¡å¼
"""

import os
import json
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()


class ChatBot:
    """é™ªè®€å°åŠ©æ‰‹å°è©±å¼•æ“"""
    
    def __init__(self, save_conversation: bool = None, use_multi_agent: bool = None):
        self.backend = os.getenv('AI_BACKEND', 'ollama').lower()
        self.save_conversation = save_conversation if save_conversation is not None else \
                                 os.getenv('SAVE_CONVERSATION', 'true').lower() == 'true'
        
        # Multi-Agent æ¨¡å¼
        self.use_multi_agent = use_multi_agent if use_multi_agent is not None else \
                               os.getenv('USE_MULTI_AGENT', 'true').lower() == 'true'
        
        # ç³»çµ±æç¤ºè©ï¼šå®šç¾©æ©Ÿå™¨äººçš„è§’è‰²ï¼ˆå–®ä¸€ Agent æ¨¡å¼ï¼‰
        self.system_instruction = """ä½ æ˜¯ä¸€å€‹æº«æŸ”ã€æœ‰è€å¿ƒçš„é™ªè®€å°åŠ©æ‰‹ã€‚
ä½ çš„ä»»å‹™æ˜¯é™ªä¼´ 5-12 æ­²çš„å°æœ‹å‹é–±è®€å’Œå­¸ç¿’ã€‚

å›ç­”åŸå‰‡ï¼š
1. ç”¨æ·ºé¡¯æ˜“æ‡‚çš„èªè¨€,é¿å…éæ–¼è‰±æ·±çš„è©å½™
2. å¤šç”¨æ¯”å–»å’Œç”Ÿæ´»åŒ–çš„ä¾‹å­
3. é¼“å‹µå°æœ‹å‹æ€è€ƒ,ä¸ç›´æ¥çµ¦ç­”æ¡ˆ
4. ä¿æŒæ­£å‘ã€é¼“å‹µçš„æ…‹åº¦
5. å›ç­”ç°¡æ½”,æ¯æ¬¡ä¸è¶…é 100 å­—
6. å¦‚æœæ˜¯å±éšªæˆ–ä¸é©ç•¶çš„å•é¡Œ,æº«å’Œåœ°å¼•å°åˆ°æ­£ç¢ºæ–¹å‘

é‡è¦ï¼šæ•¸å­¸å…¬å¼ç¬¦è™Ÿï¼ˆå¦‚ \\div, \\times, \\[ \\] ç­‰ï¼‰è¦æ”¹æˆç”¨æ–‡å­—æè¿°ï¼šã€Œ10 é™¤ä»¥ 5 ç­‰æ–¼ 2ã€è€Œä¸æ˜¯ã€Œ10 Ã· 5 = 2ã€"""
        
        # å°è©±æ­·å²
        self.chat_history = []
        
        # æ—¥èªŒç›®éŒ„
        self.log_dir = Path(os.getenv('DATA_DIR', './data')) / 'logs'
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å°æ‡‰çš„å¾Œç«¯
        if self.backend == 'ollama':
            self._init_ollama()
        elif self.backend == 'gemini':
            self._init_gemini()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ AI å¾Œç«¯: {self.backend}")
        
        # åˆå§‹åŒ– Multi-Agent ç³»çµ±ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.use_multi_agent:
            self._init_multi_agent()
    
    def _init_ollama(self):
        """åˆå§‹åŒ– Ollama å¾Œç«¯"""
        try:
            import ollama
            self.client = ollama.Client(host=os.getenv('OLLAMA_HOST', 'http://localhost:11434'))
            self.model_name = os.getenv('OLLAMA_MODEL', 'llama3.2:3b')
            
            # æ¸¬è©¦é€£ç·š
            try:
                self.client.list()
                print(f"âœ… ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹: {self.model_name}")
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•é€£ç·šåˆ° Ollama: {e}")
                print("ğŸ’¡ è«‹ç¢ºèª Ollama å·²å•Ÿå‹•: ollama serve")
                raise
                
        except ImportError:
            print("âŒ è«‹å…ˆå®‰è£ ollama: pip install ollama")
            raise
    
    def _init_gemini(self):
        """åˆå§‹åŒ– Gemini å¾Œç«¯"""
        try:
            from google import genai
            from google.genai import types
            
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("è«‹è¨­å®š GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸")
            
            self.client = genai.Client(api_key=api_key)
            self.model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
            self.types = types
            
            print(f"âœ… ä½¿ç”¨ Gemini é›²ç«¯æ¨¡å‹: {self.model_name}")
            
        except ImportError:
            print("âŒ è«‹å…ˆå®‰è£ google-genai: pip install google-genai")
            raise
    
    def _init_multi_agent(self):
        """åˆå§‹åŒ– Multi-Agent ç³»çµ±"""
        try:
            # å°‡ src ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
            src_dir = Path(__file__).parent.parent
            if str(src_dir) not in sys.path:
                sys.path.insert(0, str(src_dir))
            
            from agents import MultiAgentOrchestrator
            
            self.orchestrator = MultiAgentOrchestrator(self.client)
            print("âœ… Multi-Agent æ¨¡å¼å·²å•Ÿç”¨")
            
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è¼‰å…¥ Multi-Agent ç³»çµ±: {e}")
            print("ğŸ’¡ å°‡ä½¿ç”¨å–®ä¸€ Agent æ¨¡å¼")
            self.use_multi_agent = False
            self.orchestrator = None
    
    def chat(self, user_message: str, verbose: bool = False) -> str:
        """
        èˆ‡ AI å°è©±
        
        Args:
            user_message: ä½¿ç”¨è€…è¼¸å…¥çš„è¨Šæ¯
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°æ—¥èªŒ
            
        Returns:
            AI çš„å›æ‡‰
        """
        try:
            # ä½¿ç”¨ Multi-Agent æ¨¡å¼
            if self.use_multi_agent and self.orchestrator:
                # é¡¯ç¤ºè·¯ç”±éç¨‹
                if verbose or os.getenv('SHOW_ROUTING', 'false').lower() == 'true':
                    print(f"\n{'='*60}")
                    print(f"ğŸ” [è·¯ç”±åˆ†æ] å•é¡Œ: {user_message}")
                
                response = self.orchestrator.process_question(user_message, verbose=verbose)
                
                # é¡¯ç¤ºä½¿ç”¨çš„ Agent
                if verbose or os.getenv('SHOW_ROUTING', 'false').lower() == 'true':
                    last_agent = self.orchestrator.context.get('last_agent')
                    print(f"âœ… [ä½¿ç”¨ Agent] {last_agent}")
                    print(f"{'='*60}\n")
                
                # å„²å­˜å°è©±è¨˜éŒ„
                if self.save_conversation:
                    self._save_log(user_message, response, agent='multi-agent')
                
                return response
            
            # ä½¿ç”¨å–®ä¸€ Agent æ¨¡å¼
            if self.backend == 'ollama':
                return self._chat_ollama(user_message)
            elif self.backend == 'gemini':
                return self._chat_gemini(user_message)
                
        except Exception as e:
            print(f"âŒ AI å°è©±éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨æœ‰é»ç´¯äº†ï¼Œç­‰ä¸€ä¸‹å†èŠå¥½å—ï¼Ÿ"
    
    def _chat_ollama(self, user_message: str) -> str:
        """ä½¿ç”¨ Ollama å°è©±ï¼ˆå–®ä¸€ Agentï¼‰"""
        # å»ºç«‹å®Œæ•´çš„å°è©±è¨Šæ¯
        messages = []
        
        # åŠ å…¥ç³»çµ±æŒ‡ç¤ºï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡ï¼‰
        if not self.chat_history:
            messages.append({
                'role': 'system',
                'content': self.system_instruction
            })
        
        # åŠ å…¥æ­·å²å°è©±
        messages.extend(self.chat_history)
        
        # åŠ å…¥ç•¶å‰è¨Šæ¯
        messages.append({
            'role': 'user',
            'content': user_message
        })
        
        # å‘¼å« Ollama API
        response = self.client.chat(
            model=self.model_name,
            messages=messages
        )
        
        ai_response = response['message']['content']
        
        # æ›´æ–°å°è©±æ­·å²
        self.chat_history.append({
            'role': 'user',
            'content': user_message
        })
        self.chat_history.append({
            'role': 'assistant',
            'content': ai_response
        })
        
        # å„²å­˜å°è©±è¨˜éŒ„
        if self.save_conversation:
            self._save_log(user_message, ai_response)
        
        return ai_response
    
    def _chat_gemini(self, user_message: str) -> str:
        """ä½¿ç”¨ Gemini å°è©±ï¼ˆå–®ä¸€ Agentï¼‰"""
        # å»ºç«‹å®Œæ•´çš„å°è©±å…§å®¹
        contents = []
        
        # åŠ å…¥ç³»çµ±æŒ‡ç¤ºï¼ˆä½œç‚ºç¬¬ä¸€æ¢è¨Šæ¯ï¼‰
        if not self.chat_history:
            contents.append(self.types.Content(
                role='user',
                parts=[self.types.Part(text=self.system_instruction)]
            ))
            contents.append(self.types.Content(
                role='model',
                parts=[self.types.Part(text='å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ï¼æˆ‘æœƒç”¨æ·ºé¡¯æ˜“æ‡‚çš„æ–¹å¼é™ªä¼´å°æœ‹å‹å­¸ç¿’ã€‚')]
            ))
        
        # åŠ å…¥æ­·å²å°è©±
        for msg in self.chat_history:
            contents.append(self.types.Content(
                role=msg['role'] if msg['role'] != 'assistant' else 'model',
                parts=[self.types.Part(text=msg['content'])]
            ))
        
        # åŠ å…¥ç•¶å‰è¨Šæ¯
        contents.append(self.types.Content(
            role='user',
            parts=[self.types.Part(text=user_message)]
        ))
        
        # å‘¼å« Gemini API
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=contents
        )
        
        ai_response = response.text
        
        # æ›´æ–°å°è©±æ­·å²
        self.chat_history.append({
            'role': 'user',
            'content': user_message
        })
        self.chat_history.append({
            'role': 'assistant',
            'content': ai_response
        })
        
        # å„²å­˜å°è©±è¨˜éŒ„
        if self.save_conversation:
            self._save_log(user_message, ai_response)
        
        return ai_response
    
    def _save_log(self, user_msg: str, ai_msg: str, agent: str = 'single'):
        """å„²å­˜å°è©±è¨˜éŒ„åˆ°æ—¥èªŒæª”æ¡ˆ"""
        timestamp = datetime.now().strftime("%Y%m%d")
        log_file = self.log_dir / f"conversation_{timestamp}.jsonl"
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user": user_msg,
            "assistant": ai_msg,
            "backend": self.backend,
            "model": self.model_name,
            "mode": "multi-agent" if self.use_multi_agent else "single-agent",
            "agent_used": agent
        }
        
        if self.use_multi_agent and self.orchestrator:
            log_entry['last_agent'] = self.orchestrator.context.get('last_agent')
        
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def reset_conversation(self):
        """é‡ç½®å°è©±æ­·å²"""
        self.chat_history = []
        
        if self.use_multi_agent and self.orchestrator:
            self.orchestrator.reset_context()
        
        print("âœ… å°è©±æ­·å²å·²æ¸…é™¤")
